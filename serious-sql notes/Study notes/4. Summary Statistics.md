

## Summary statistics (Singular point statistics)
Used to find data issues, inspect them, and remove outliers to get meaningful insights.
Tips:
* Look at median first, then compare with mean - get a sense if there are outliers
* Best practice is not to look at `MODE` but use `GROUP BY COUNT` to see frequently occuring values. Because you can actually have 1 or more mode values in a dataset. For example, say 40 and 60 both appear three times (more than any other value), then the modes of our dataset would be 40 and 60.

| Summary Stats Output |
| -------------------- |
| minimum\_value       |
| maximum\_value       |
| mean\_value          |
| median\_value        |
| mode\_value          |
| standard\_deviation  |
| variance             |

### Central tendency
* Mean/Average `AVG(col_name)`
* Median (50th percentile) `PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY col_name)`
* Mode (most frequent value, similar to `COUNT(*)` and `GROUP BY`) `MODE() WITHIN GROUP (ORDER BY col_name)`

### Spread statistics
* `MIN`, `MAX`, range (`MIN` - `MAX`)
* Variance & standard deviation - What is the deviation/spread of data from the mean? `VARIANCE(col_name)` and `STDDEV(col_name)` (**use STDDEV when communicating insights, as variance is too large**)

### Confidence intervals / Empirical rules
* 68% of values lie in the range of mean +- 1*stddev
* 95% of values lie in the range of mean +- 2*stddev
* 99.7% of values lie in the range of mean +- 3*stddev

## Cumulative distribution functions (Percentiles, bell curve)
To find outliers, we can simply use `SELECT`, `ORDER BY`, `LIMIT`. But by inspecting percentiles, we have more insights about the outliers.

Data algorithm for range distribution values and percentiles:
1. Sort values ascendingly, then Assign 1 - 100 percentile - `NTILE(100) OVER (ORDER BY col_name)` (`NTILE(10)` is also common in DS world)
2. For each percentile (`GROUP BY`):
* Calculate floor and ceiling values (`MIN`, `MAX`)
* Calculate record counts (`COUNT(*)`)

| percentile | floor\_value | ceiling\_value | percentile\_counts |
| ---------- | ------------ | -------------- | ------------------ |
| 1          | 0            | 15             | 45                 |
| 2          | 16           | 20             | 15                 |
| 3          | 21           | 25             | 2                  |
| 4          | 26           | 30             | 28                 |
| 5          | 31           | 35             | 6                  |

3. Inspect outliers & extreme values by selecting `percentile = 1` & `percentile = 100` values
4. Use ranking functions to sort values for 1st and 100th percentile
* `ROW_NUMBER`
* `RANK`
* `DENSE_RANK` (diff to `RANK` in the sense that there is no skipping, hence "dense")
5. Remove outliers with `WHERE` filter, create temp table for further data analysis
6. Run queries on treated dataset without outliers to get updated summary statistics and percentiles

## Frequency distribution (Histogram - use height of bars to represent how many records exist in a range instead of using bell curve)
1. Find the min, max boundary values
2. Split into N equal buckets by values `WIDTH_BUCKET(col_name, min, max, number_of_bins)`
3. For each bucket (`GROUP BY`):
* Calculate average value `AVG`
* Calculate record count `COUNT(*)`

### Cumulative distribution chart or histogram/frequency plot?
You can make both. Cumulative chart is more valuable. Histogram will look different depending on the number of bins, used when communicating with non-technical stakeholders
